{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Import the spam dataset and print the first six rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam_dataset.csv')\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose the following three variables as the predictors in the model of spam:\n",
    "- capital_run_length_average:\n",
    "- word_freq_free:\n",
    "- char_freq_!:\n",
    "\n",
    "Because spam e-mail, especially advertisement emails are full of \"!\", 'free\" and capitalized words, so I choose these three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>word_freq_free:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.756</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.114</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.821</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.537</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.537</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   capital_run_length_average:  word_freq_free:  char_freq_!:  spam\n",
       "0                        3.756             0.32         0.778     1\n",
       "1                        5.114             0.14         0.372     1\n",
       "2                        9.821             0.06         0.276     1\n",
       "3                        3.537             0.31         0.137     1\n",
       "4                        3.537             0.31         0.135     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3 = df[['capital_run_length_average:','word_freq_free:','char_freq_!:','spam']]\n",
    "df_3.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Visualize the univariate distribution of each of the variables in the previous question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'spam')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes=plt.subplots(nrows=1,ncols=4,figsize=(16,6))\n",
    "\n",
    "axes[0].hist(df_3['capital_run_length_average:'])\n",
    "axes[0].set_title('capital_run_length_average')\n",
    "axes[1].hist(df_3['word_freq_free:'])\n",
    "axes[1].set_title('word_freq_free')\n",
    "axes[2].hist(df_3['char_freq_!:'])\n",
    "axes[2].set_title('char_freq_!')\n",
    "axes[3].hist(df_3['spam'])\n",
    "axes[3].set_title('spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learnt 6 models to predict classification problems.\n",
    "- k-Nearest-Neighbors Classifier\n",
    "- Logistic Regression (as well as penalized Logistic Regression) \n",
    "- Support Vector Machine\n",
    "- Decision Tree\n",
    "- Bagged Trees\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Describe the importance of training and test data.  Why do we separate data into these subsets?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental purpose to split data into training and test set is to avoid overfitting. All Machine Learning Algorithms face the same trade-off between variance and bias. When we want to increase models' fit to training dataset, we can lower the bias, but face the problem of high variance, and this problem is called overfit. Overfit can significantly reduce the predictability of a model. Hence, we seperate the data first, to test if the model can give good prediction on test data, to avoid such overfitting models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is k-fold cross validation and what do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross validation involves randomly dividing the set of observations into k groups of approximately equal size. One fold is treated as a validation set, and the method is ﬁt on the remaining k − 1 folds. Then the predictability (using metrics such as mean squared error for regression, or accuracy for classification) is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a diﬀerent group of observations is treated as a validation set.\n",
    "\n",
    "This method is used to exam the test error rate of the model, thus avoiding the potential overfitting problems of our model. What's more, compared to Leave-One-Out Cross Validation, which may lead to computational intensivity when the sample is large, k-fold CV can significantly reduce computational intensivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified k-fold cross validation a kind of k-fold CV thatis designed to correct a potential problem raised by k-fold cross validaton - imbalanced data. They are only different on one perspective: Stratification. When we divide data into k flods, Stratified k-fold cross validation requires to use stratified sampling to ensure that each fold of dataset has the same proportion of observations with a given label, thusing avoid the problem of imbalanced data. On the contrary, k-fold CV only does simple random division, which may lead to imbalanced data in some folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  \n",
    "- Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  \n",
    "- Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = df_3.loc[:, df_3.columns != 'spam']\n",
    "y = df_3['spam']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION (SCALED DATA)\n",
      "best parameter of Logistic Regression: {'C': 2.7}\n",
      "best cross-validation score of Logistic Regression: 0.797\n",
      "Test set score: 0.798\n"
     ]
    }
   ],
   "source": [
    "# Penalized Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set stratified k-folds\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle = True, random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best model \n",
    "C = {'C': np.arange(0.1, 5, 0.1)}\n",
    "grid_logit = GridSearchCV(LogisticRegression(),param_grid=C, cv=skfold)\n",
    "grid_logit.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"best parameter of Logistic Regression: {}\".format(grid_logit.best_params_))\n",
    "print(\"best cross-validation score of Logistic Regression: {:.3f}\".format(grid_logit.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_logit.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 8:\n",
    "- use GridSearchCV to iterate the parameters from 0.1 to 5.0 to find the best parameter for model\n",
    "- the prediction error using cross-validation is 1-0.797 = 0.203\n",
    "- the prediction error using test set is 1-0.798 = 0.202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Choose a second model from question four.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC (SCALED DATA)\n",
      "best parameter of SVC: {'C': 10, 'gamma': 10, 'kernel': 'rbf'}\n",
      "best cross-validation score of SVC: 0.848\n",
      "Test set score: 0.846\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Use GridSearchCV to find the best model \n",
    "param_grid = {'C': [0.1,1, 10, 100, 1000], \n",
    "              'gamma': [10,1,0.1,0.01,0.001,0.0001], \n",
    "              'kernel': ['rbf']} \n",
    "\n",
    "grid_SVC = GridSearchCV(SVC(),param_grid=param_grid, cv=skfold)\n",
    "grid_SVC.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(\"SVC (SCALED DATA)\")\n",
    "print(\"best parameter of SVC: {}\".format(grid_SVC.best_params_))\n",
    "print(\"best cross-validation score of SVC: {:.3f}\".format(grid_SVC.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_SVC.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 9:\n",
    "- use GridSearchCV to iterate the parameters C from 0.1 to 1000, and parameter gamma from 0.0001 to 10, to find the best parameter for model\n",
    "- the prediction error using cross-validation is 1-0.848 = 0.152\n",
    "- the prediction error using test set is 1-0.846 = 0.154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Choose a third model from question four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (UNSCALED DATA)\n",
      "best parameter of Decision Tree: {'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 2}\n",
      "best cross-validation score of Decision Tree: 0.849\n",
      "Test set score: 0.853\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Use GridSearchCV to find the best model \n",
    "max_depth = np.arange(1, 10, 1)\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "criterion = ['entropy','gini']\n",
    "\n",
    "param_grid = {'max_depth' : max_depth,\n",
    "             'min_samples_leaf' : min_samples_leaf,\n",
    "             'criterion' : criterion} \n",
    "\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(),param_grid=param_grid, cv=skfold)\n",
    "grid_tree.fit(X_train, y_train) \n",
    "\n",
    "print(\"Decision Tree (UNSCALED DATA)\")\n",
    "print(\"best parameter of Decision Tree: {}\".format(grid_tree.best_params_))\n",
    "print(\"best cross-validation score of Decision Tree: {:.3f}\".format(grid_tree.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 10:\n",
    "- use GridSearchCV to iterate the parameters (including max_depth (number of times internal nodes are split), min_samples_leaf(threshold for minimum number of observations per terminal node) and criterion), to find the best parameter for model.\n",
    "- the prediction error using cross-validation is 1-0.849 = 0.151\n",
    "- the prediction error using test set is 1-0.853 = 0.147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.  Choose a fourth model from question four.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (SCALED DATA)\n",
      "best parameter of KNN Classifier: {'n_neighbors': 7}\n",
      "best cross-validation score of KNeighborsClassifier: 0.849\n",
      "Test set score: 0.839\n"
     ]
    }
   ],
   "source": [
    "#KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 10, 1)}\n",
    "# Use GridSearchCV to find the best model \n",
    "\n",
    "grid_knn = GridSearchCV(KNeighborsClassifier(),param_grid=param_grid, cv=skfold)\n",
    "grid_knn.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(\"KNN (SCALED DATA)\")\n",
    "print(\"best parameter of KNN Classifier: {}\".format(grid_knn.best_params_))\n",
    "print(\"best cross-validation score of KNeighborsClassifier: {:.3f}\".format(grid_knn.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 11:\n",
    "- use GridSearchCV to iterate the parameters n, to find the best parameter for model.\n",
    "- the prediction error using cross-validation is 1-0.849 = 0.151\n",
    "- the prediction error using test set is 1-0.839 = 0.161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### based on the preformance of four models, Decision Tree has the best test accuracy and cross-validation score.\n",
    "\n",
    "add \"word_freq_george\", \"word_freq_650\" and \"char_freq_(:\" to current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (UNSCALED DATA)\n",
      "best cross-validation score of Decision Tree: 0.861\n",
      "Test set score: 0.859\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree (Best Model)\n",
    "\n",
    "#add \"word_freq_george\", \"word_freq_650\" and \"char_freq_(:\" to current model\n",
    "X = df[[\"capital_run_length_average:\",\"word_freq_free:\",\"char_freq_!:\",\n",
    "        'word_freq_george:',\"word_freq_650:\",\"char_freq_(:\"]]\n",
    "y = df['spam']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "\n",
    "# Use GridSearchCV to find the best model \n",
    "max_depth = np.arange(1, 10, 1)\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "criterion = ['entropy','gini']\n",
    "\n",
    "param_grid = {'max_depth' : max_depth,\n",
    "             'min_samples_leaf' : min_samples_leaf,\n",
    "             'criterion' : criterion} \n",
    "\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(),param_grid=param_grid, cv=skfold)\n",
    "grid_tree.fit(X_train, y_train) \n",
    "\n",
    "print(\"Decision Tree (UNSCALED DATA)\")\n",
    "print(\"best cross-validation score of Decision Tree: {:.3f}\".format(grid_tree.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 12:\n",
    "- The model does a better job in predicting test dataset, increasing the accuracy from 0.852 to 0.862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprossessing the data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION (SCALED DATA)\n",
      "best cross-validation score of Logistic Regression: 0.815\n",
      "Test set score: 0.811\n"
     ]
    }
   ],
   "source": [
    "# Penalized Regression Model\n",
    "C = {'C': np.arange(0.1, 5, 0.1)}\n",
    "grid_logit = GridSearchCV(LogisticRegression(),param_grid=C, cv=skfold)\n",
    "grid_logit.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"best cross-validation score of Logistic Regression: {:.3f}\".format(grid_logit.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_logit.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC (SCALED DATA)\n",
      "best cross-validation score of SVC: 0.855\n",
      "Test set score: 0.850\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "param_grid = {'C': [0.1,1, 10, 100, 1000], \n",
    "              'gamma': [10,1,0.1,0.01,0.001,0.0001], \n",
    "              'kernel': ['rbf']} \n",
    "\n",
    "grid_SVC = GridSearchCV(SVC(),param_grid=param_grid, cv=skfold)\n",
    "grid_SVC.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(\"SVC (SCALED DATA)\")\n",
    "print(\"best cross-validation score of SVC: {:.3f}\".format(grid_SVC.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_SVC.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (SCALED DATA)\n",
      "best cross-validation score of KNeighborsClassifier: 0.863\n",
      "Test set score: 0.862\n"
     ]
    }
   ],
   "source": [
    "#KNN Classifier\n",
    "param_grid = {'n_neighbors': np.arange(1, 10, 1)}\n",
    "\n",
    "grid_knn = GridSearchCV(KNeighborsClassifier(),param_grid=param_grid, cv=skfold)\n",
    "grid_knn.fit(X_train_scaled, y_train) \n",
    "\n",
    "print(\"KNN (SCALED DATA)\")\n",
    "print(\"best cross-validation score of KNeighborsClassifier: {:.3f}\".format(grid_knn.best_score_))\n",
    "print(\"Test set score: {:.3f}\".format(grid_knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 13\n",
    "- I choose Decision Tree. Among all the models, Decision Tree outperform others by providing the best test data prediction accuracy and highest cross-validation score. What's more, it does not require scaling the data, and it has a fast processing speed. That's why I choose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the frequency of word \"sale\", might be a good predictor of spam emails. Because spam emails are predominantly advertisement email, so \"sale\" should be a highly repeated word in these e-mails, thus making it a good predictor of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest-Neighborhood Regression\n",
    "- Linear Regression (including Ridge and Lasso)\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Bagged Trees\n",
    "- Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
